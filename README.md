# Talks by Jesper Dramsch

## All Talks







<!-- TALKS -->
### Increase citations, ease review & collaboration – Making machine learning in research reproducible

| Event | Location | Date | Link | Video | Slides |
| ----- | -------- | ---- | ---- | ----- | ------ |
| Euroscipy 2022 | Basel, Switzerland | 2022-09-01 | [Euroscipy](https://pretalx.com/euroscipy-2022/talk/8RAJX7/) | | [Slides](2022-09-01%20Euroscipy/) |

> Every scientific conference has seen a massive uptick in applications that use some type of machine learning. Whether it’s a linear regression using scikit-learn, a transformer from Hugging Face, or a custom convolutional neural network in Jax, the breadth of applications is as vast as the quality of contributions.
>
> This tutorial aims to provide easy ways to increase the quality of scientific contributions that use machine learning methods. The reproducible aspect will make it easy for fellow researchers to use and iterate on a publication, increasing citations of published work. The use of appropriate validation techniques and increase in code quality accelerates the review process during publication and avoids possible rejection due to deficiencies in the methodology. Making models, code and possibly data available increases the visibility of work and enables easier collaboration on future work.
>
> This work to make machine learning applications reproducible has an outsized impact compared to the limited additional work that is required using existing Python libraries.
### How to Guarantee No One Understands What You Did in Your Machine Learning Project

| Event | Location | Date | Link | Video | Slides |
| ----- | -------- | ---- | ---- | ----- | ------ |
| Pydata Global 2021 | Online | 2021-10-30 | [Pydata](https://pydata.org/global2021/schedule/presentation/112/how-to-guarantee-no-one-understands-what-you-did-in-your-machine-learning-project/) | [Video](https://www.youtube.com/watch?v=ucgCGGb088E) | [Slides](2021-10-28%20Pydata%20Global/) |

> Data science and machine learning can be a lot of fun. Freshly out of university, a bootcamp, or through the grinder of a Kaggle competition, we learned all the neat technical tricks. Suddenly that's only a basic requirement to get a job or even make anyone interested in your machine learning project. Let's dissect that.
>
> Working on a model for a few days, researching the architecture, perfecting all the tweaks and fixing all the leakage can make us forget that we're the only one that deep into the material. I sure do. But technobabble will not get us anywhere. The problem is that no one else is that deep into it or necessarily even cares. Depending on the company you keep, it may even be worse. I'm originally a geophysicist. Among physicists, you have a large number of people highly sceptical of machine learning. Talking too much about the specifics of the machine learning model may be actively detrimental. Funnily, sometimes we have the completely opposite problem, like I do at my new job. I'm amongst people who run circles around me on the topic of classic statistics. Yet, and this may come as a surprise, they know less than me about machine learning.
>
> So how do we fix that? How do we scale that gap? (And how do we make this into an interesting Talk?)
>
> Communicating machine learning and data science means building trust with your stakeholders. (Be they your job interviewer, grant coordinator, colleagues, or management at a job.)
>
> There's a specific time and place for talking shop, and that's when you're with other mechanics. In this talk, I will dive into the more abstract understanding of setting expectations around machine learning with stakeholders in the beginning. Then we'll progress into tools and strategies to communicate machine learning results to people who don't necessarily care about machine learning.
>
> We'll explore specific tools from machine learning explainability, interpretability and touch on causality. We'll talk about ethical considerations. We'll explore helpful visualizations and tools for interactivity. Finally, we talk about model validation specific to different expert domains and tie it all together.
<!-- //TALKS -->







## Tech

Marp based: https://marp.app/#get-started
